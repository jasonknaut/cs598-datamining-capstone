{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Temporal Brand & Product Perception Discovery from Online Reviews with Topical Phrase Mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_short, remove_stopwords, strip_punctuation, strip_numeric\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyarrow import json\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from sklearn import preprocessing\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data -- raw Amazon Reviews 2018 data sets\n",
    "RAW_REVIEWS_FILE='./data/Cell_Phones_and_Accessories.json.gz'\n",
    "RAW_REVIEWS_METADATA_FILE='./data/meta_Cell_Phones_and_Accessories.json.gz'\n",
    "CATEGORY='Cell Phones'\n",
    "\n",
    "# Configuration for processed data\n",
    "REVIEWS_FILE = './data/cleaned_reviews.json'\n",
    "TOPICS_DIR = './data/topics'\n",
    "CORES = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Multiprocessing Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel(fn, arg):\n",
    "    if CORES == 1:\n",
    "        result = fn(arg)\n",
    "    else:\n",
    "        pool = multiprocessing.Pool(CORES-1)\n",
    "        result = pool.map(fn, arg)\n",
    "        pool.terminate()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def filter_parallel(fn, arg):        \n",
    "    if CORES == 1:\n",
    "        result = [c for c, check in zip(arg, fn(arg)) if check]\n",
    "    else:\n",
    "        pool = multiprocessing.Pool(CORES-1)\n",
    "        result = [c for c, check in zip(arg, pool.map(fn, arg)) if check]\n",
    "        pool.terminate()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the Raw Reviews Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the Spark Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .appName(\"Product Reviews\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get the Products Associated with Each Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products(category):\n",
    "    \"\"\" Returns a dataframe with all mobile phone products \"\"\"\n",
    "    df = spark.read.json(RAW_REVIEWS_METADATA_FILE)\n",
    "    \n",
    "    products_df = df.select(\"asin\",\n",
    "                       F.lower(df.brand).alias(\"brand\"), \n",
    "                       F.lower(df.title).alias(\"product\"),\n",
    "                       F.explode(\"category\").alias(\"category\")) \\\n",
    "               .filter(f\"category = '{category}'\") \\\n",
    "               .drop(\"category\") \\\n",
    "               .filter(\"brand is not null\")\n",
    "    \n",
    "    return products_df\n",
    "\n",
    "\n",
    "def get_products_by_brand(df):\n",
    "    \"\"\" Returns the number of unique SKUs per Brand \"\"\"\n",
    "    brands_df = df.select(\"brand\") \\\n",
    "                  .groupby(\"brand\") \\\n",
    "                  .count() \\\n",
    "                  .withColumnRenamed(\"count\", \"num_products\") \\\n",
    "                  .sort(F.col(\"num_products\").desc())\n",
    "    \n",
    "    return brands_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Get the reviews for the desired category and filter to most reviewed brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(products_df):\n",
    "    reviews_df = spark.read.json(RAW_REVIEWS_FILE)\n",
    "    reviews_df = reviews_df.select(\"asin\", \"overall\", \"reviewText\", \"reviewTime\", \"vote\") \\\n",
    "                           .filter(\"reviewText is not null\") \\\n",
    "                           .withColumn(\"reviewYear\", F.substring(F.col(\"reviewTime\"), -4,4).cast(IntegerType())) \\\n",
    "                           .join(products_df, \"asin\") \n",
    "    return reviews_df\n",
    "\n",
    "\n",
    "def get_pop_brand_reviews(reviews_df, min_reviews=1000):\n",
    "    \"\"\" Returns reviews for only the popular brands \"\"\"\n",
    "    brands_df = reviews_df.groupby(\"brand\") \\\n",
    "                          .count().sort(F.col(\"count\").desc()) \\\n",
    "                          .where(f\"count >= {min_reviews}\")\n",
    "    reviews_df = reviews_df.join(brands_df, \"brand\").drop(\"count\")\n",
    "    \n",
    "    return brands_df, reviews_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get brand/product meta information\n",
    "products_df = get_products(CATEGORY)\n",
    "\n",
    "# Get reviews for the desired category of products\n",
    "reviews_df = get_reviews(products_df)\n",
    "top_brands_df, reviews_df = get_pop_brand_reviews(reviews_df, 400)\n",
    "reviews_df.cache()\n",
    "\n",
    "products_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Save the compressed, processed data set to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderedCols = [\"brand\",\"product\",\"asin\",\"reviewYear\",\"overall\",\"reviewText\"]\n",
    "clean_df = reviews_df.select(orderedCols)\\\n",
    "                     .coalesce(1)\\\n",
    "                     .sort(\"brand\",\"product\",\"asin\",\"reviewYear\",\"overall\")\\\n",
    "                     .withColumn(\"review_id\", F.monotonically_increasing_id())\n",
    "\n",
    "clean_df.write.format(\"json\")\\\n",
    "    .option(\"compression\", \"org.apache.hadoop.io.compress.GzipCodec\")\\\n",
    "    .save(\"prepared_reviews\")\n",
    "\n",
    "# Clean up Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import & Prepare Topical Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the topics from the ToPMine outputs (containing phrases and frequencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_topics(topic_dir, ext=\"txt\", min_freq=None):\n",
    "    \n",
    "    # get topic files\n",
    "    file_pattern = os.path.join(topic_dir, f\"*.{ext}\")\n",
    "    topic_files = glob.glob(file_pattern)\n",
    "    \n",
    "    # load topic phrases into dataframe\n",
    "    col_names = [\"phrase\", \"freq\", \"topic\"]\n",
    "    phrases_df = pd.DataFrame(columns=col_names)\n",
    "    \n",
    "    for i, f in enumerate(topic_files):\n",
    "        df = pd.read_csv(f, sep=\"\\t\", header=None, names=col_names[:2])\n",
    "        df['topic'] = i\n",
    "        \n",
    "        if min_freq:\n",
    "            df = df[df['freq'] > min_freq]\n",
    "        phrases_df = phrases_df.append(df, ignore_index=True)\n",
    "        \n",
    "    phrases_df['freq'] = phrases_df['freq'].astype(int)\n",
    "    \n",
    "    return phrases_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because phrases may be in multiple topics, assign a phrase to the dominant phrase (i.e. ensure one-to-one relationship between phrases and topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_phrase(phrase):\n",
    "    MIN_TOKEN_LEN = 2\n",
    "    \n",
    "    phrase = phrase.lower()\n",
    "    phrase = strip_short(phrase, MIN_TOKEN_LEN)\n",
    "    phrase = preprocess_string(phrase, filters=[strip_numeric])\n",
    "    \n",
    "    phrase = [w for w in phrase if w not in stopwords.words('english')]\n",
    "    \n",
    "    phrase = ' '.join(phrase) if phrase else None\n",
    "                    \n",
    "    return phrase   \n",
    "\n",
    "def unique_phrases(df, f=preprocess_phrase):\n",
    "    \"\"\" Each phrase may only appear in one topic \"\"\"\n",
    "    \n",
    "    # preprocess phrases\n",
    "    df['phrase'] = df['phrase'].map(f)\n",
    "    \n",
    "    #combine phrases by topic\n",
    "    df = df.groupby(['phrase','topic'], as_index=False)['freq'].sum()\n",
    "    \n",
    "    # assign to topic where phrase is most frequently seen\n",
    "    df['rank'] = df.groupby(['phrase'])['freq'].rank(ascending=False)\n",
    "    df = df[df['rank']==1]\n",
    "    \n",
    "    # clean up dataframe\n",
    "    df = df.drop(columns=['rank'])\n",
    "    df = df.sort_values(['topic','freq'], ascending=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def top_phrases(df, max_topic_size, topic_col='topic', phrase_col='phrase', freq_col='freq'):\n",
    "    \"\"\" Limit the number of candidate phrases in each topic\"\"\"\n",
    "    \n",
    "    # rank by freq and keep top 'max_topic_size' freq phrases per topic\n",
    "    df['rank'] = df.groupby([topic_col])[freq_col].rank(ascending=False)\n",
    "    df = df[df['rank'] <= max_topic_size]\n",
    "    \n",
    "    # clean up dataframe\n",
    "    df = df.drop('rank',axis=1)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform topics into sets of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_to_sets(df, topic_col='topic', phrase_col='phrase'):\n",
    "    num_topics = df[topic_col].max()+1\n",
    "    phrase_sets = []\n",
    "    \n",
    "    for i in range(0, num_topics):\n",
    "        topic_i = set(df[df[topic_col] == i][phrase_col])\n",
    "        phrase_sets.append(topic_i)\n",
    "    \n",
    "    return phrase_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all topic phrases into sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PHRASE_FREQ = 50\n",
    "MAX_TOPIC_SIZE = 500\n",
    "\n",
    "topics_df = load_topics(TOPICS_DIR, min_freq=MIN_PHRASE_FREQ)\n",
    "phrases_df = unique_phrases(topics_df)\n",
    "phrases_df = top_phrases(phrases_df, MAX_TOPIC_SIZE)\n",
    "phrase_sets = topics_to_sets(phrases_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocab datastructure will aid in both compression and efficient lookup of topical phrases for later computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicVocab:\n",
    "    \n",
    "    def __init__(self, df, topic_col='topic', phrase_col='phrase'):\n",
    "        self.__vocab, self.__inverse_vocab = self.__create_vocab(df, phrase_col)\n",
    "        self.__topic_vocab = self.__create_topic_vocab(df, topic_col, phrase_col)\n",
    "        self.__topics = list(df[topic_col])\n",
    "        self.num_topics = len(set(self.__topics))\n",
    "        \n",
    "    def __create_vocab(self, df, phrase_col):\n",
    "        vocab = list(df[phrase_col])\n",
    "        inverse_vocab = {}\n",
    "        for idx, phrase in enumerate(vocab):\n",
    "            inverse_vocab[phrase] = idx\n",
    "        return vocab, inverse_vocab\n",
    "    \n",
    "    def __create_topic_vocab(self, df, topic_col, phrase_col):\n",
    "        topics = list(phrases_df[topic_col].unique())\n",
    "        topic_vocab = {}\n",
    "        \n",
    "        for t in topics:\n",
    "            phrase_ids = list(df[df[topic_col]==t].index)\n",
    "            topic_vocab[t]= phrase_ids\n",
    "            \n",
    "        return topic_vocab\n",
    "            \n",
    "    def get_phrase_id(self, phrase):\n",
    "        return self.__inverse_vocab[phrase]\n",
    "    \n",
    "    def get_topic_id(self, phrase):\n",
    "        phrase_id = self.__inverse_vocab[phrase]\n",
    "        return self.__topics[phrase_id]\n",
    "    \n",
    "    def phrase_ids(self, topic):\n",
    "        return set(self.__topic_vocab[topic])\n",
    "    \n",
    "    def phrases(self, topic):\n",
    "        topic_vocab = self.__topic_vocab[topic]\n",
    "        phrases = [self[idx] for idx in topic_vocab]\n",
    "        return set(phrases)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.__vocab)\n",
    "    \n",
    "    def intersection(self, other):\n",
    "        return other.intersection(set(self.__vocab))\n",
    "    \n",
    "    def __and__(self, other):\n",
    "        return self.intersection(other)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        phrase = None\n",
    "        if idx < len(self.__vocab):\n",
    "            phrase = self.__vocab[idx]\n",
    "        return phrase\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = TopicVocab(phrases_df)\n",
    "\n",
    "print(f\"The vocab contains {vocab.size()} interesting product attribute phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process the Reviews into Sentences & Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned and prepared review data is imported and then processed into sentences and phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = json.read_json(REVIEWS_FILE)\n",
    "reviews_df = reviews.to_pandas()\n",
    "del reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split each review into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = list(reviews_df['reviewText'])\n",
    "review_sentences = run_parallel(nltk.sent_tokenize, review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create candidate phrases from the review sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    MIN_TOKEN_LEN = 2\n",
    "    MAX_NGRAM = 3\n",
    "    SEP = ' '\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    sentence = strip_short(sentence, MIN_TOKEN_LEN)\n",
    "    sentence = preprocess_string(sentence, \n",
    "                                 filters=[remove_stopwords,\n",
    "                                          strip_punctuation,\n",
    "                                          strip_numeric])\n",
    "    \n",
    "    n_grams = nltk.everygrams(sentence, max_len=MAX_NGRAM)\n",
    "    n_grams = set([SEP.join(t) for t in n_grams])\n",
    "                             \n",
    "    return n_grams\n",
    "\n",
    "def preprocess_review(sentences):\n",
    "    return [preprocess_sentence(s) for s in sentences]\n",
    "\n",
    "review_ngrams = run_parallel(preprocess_review, review_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all of the candidate phrases that are discovered product attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_phrases(ngram_sets, vocab, encoding_type='phrase'):\n",
    "    \"\"\" filter to ngrams that are in the phrase vocabulary \"\"\"\n",
    "    encoders = {'phrase': vocab.get_phrase_id,\n",
    "                'topic': vocab.get_topic_id}\n",
    "    \n",
    "    phrases = [list(vocab & ngrams) for ngrams in ngram_sets]\n",
    "    encode = encoders[encoding_type]\n",
    "    \n",
    "    ids = []\n",
    "    for phrase in phrases:\n",
    "        tmp_ids = [encode(p) for p in phrase]\n",
    "        ids.append(tmp_ids)\n",
    "\n",
    "    return ids\n",
    "\n",
    "def phrase_encoder(ngrams):\n",
    "    return encode_phrases(ngrams, vocab)\n",
    "\n",
    "def topic_encoder(ngrams):\n",
    "    return encode_phrases(ngrams, vocab, encoding_type='topic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract interesting phrases from the ngrams and encode using the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_phrases = run_parallel(phrase_encoder, review_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the topic ids for the extracted phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_topics = run_parallel(topic_encoder, review_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free up memory by releasing the candidate ngrams in favor of the compressed, econded phrases/topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del review_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Scoring of Reviews & Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment scoring component computes sentence-level sentiment scores for every review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def score_sentiment(sentences):\n",
    "    scores = list(map(analyser.polarity_scores, sentences))\n",
    "    scores = [s['compound'] for s in scores]\n",
    "    return scores\n",
    "\n",
    "review_scores = run_parallel(score_sentiment, review_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the topic-specific sentiment for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_sentiment_scorer(topic, scores, topics):\n",
    "    scores_and_topics = list(zip(topics, scores))\n",
    "    sentiments = [s for (t,s) in scores_and_topics if topic in t]\n",
    "\n",
    "    n = len(sentiments)\n",
    "    avg_sent = sum(sentiments) / n if n>0 else None\n",
    "    \n",
    "    return avg_sent\n",
    "\n",
    "def parallel_topic_scorer(args):\n",
    "    topic = args[0]\n",
    "    scores = args[1]\n",
    "    topics = args[2]\n",
    "    return topic_sentiment_scorer(topic, scores, topics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = vocab.num_topics\n",
    "\n",
    "def append_topic_sentiment(df, num_topics, review_scores, review_topics):\n",
    "    num_reviews = len(review_scores)\n",
    "    \n",
    "    for i in range(num_topics):\n",
    "        args = list(zip([i]*num_reviews, review_scores, review_topics))\n",
    "        result = run_parallel(parallel_topic_scorer, args)\n",
    "        df[f\"topic_{i}_score\"] = result\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns with the sentiment scores for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = append_topic_sentiment(reviews_df, NUM_TOPICS,\n",
    "                                    review_scores, review_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the seperate 2d data structures holding the extracted phrases and topics. Also aggregate lists of phrases to distinct topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_topic_phrases(review):\n",
    "    merged = {}\n",
    "    review_id, topics, phrases = review\n",
    "\n",
    "    for i in range(len(topics)):\n",
    "        topic = topics[i]\n",
    "        phrase = phrases[i]\n",
    "        for j in range(len(topic)):\n",
    "            t = topic[j]\n",
    "            p = phrase[j]\n",
    "            if t not in merged:\n",
    "                merged[t] = set()\n",
    "            merged[t].add(p)\n",
    "\n",
    "    result = [(review_id, topic, list(phrases)) for topic, phrases in merged.items()]\n",
    "\n",
    "    return result\n",
    "\n",
    "def filter_empty(args):\n",
    "    return len(args) > 0\n",
    "\n",
    "# Merge review id, topics, phrases into tuples\n",
    "review_ids = list(reviews_df['review_id'])\n",
    "combined = zip(review_ids, review_topics, review_phrases)\n",
    "flattened_topics = run_parallel(flatten_topic_phrases, combined)\n",
    "\n",
    "# Only keep review mappings where a topic/phrase was extracted\n",
    "flattened_topics = filter_parallel(filter_empty, flattened_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the intermediate columns to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del review_topics\n",
    "del review_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe that splits the (review_id, topic, phrases) tuples into columns. Each review is expanded so that a row describes a review_id, topic combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_review_phrase_df(mappings):\n",
    "    \"\"\" Transform tuples of (review_id, topic, phrase lists) into\n",
    "        a data frame\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dataframe col with (id, topic, phrase) tuples\n",
    "    df = pd.DataFrame(columns=['tuples'])\n",
    "    df['tuples'].astype('object')\n",
    "    df['tuples'] = mappings\n",
    "    \n",
    "    # Create a unique row for each review/topic pair\n",
    "    df = df['tuples'].explode().reset_index().drop('index',axis=1)\n",
    "    df = pd.DataFrame(df['tuples'].tolist(), index=df.index, columns=['review_id','topic','phrases'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "review_phrase_df = create_review_phrase_df(flattened_topics)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary of Smartphone Review Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total Reviews by Year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_year = reviews_df.groupby('reviewYear')['review_id'] \\\n",
    "                    .count() \\\n",
    "                    .reset_index() \\\n",
    "                    .set_index('reviewYear')\n",
    "\n",
    "total_reviews = by_year['review_id'].sum()\n",
    "by_year.columns=['review_count'] \n",
    "    \n",
    "print(by_year)\n",
    "print(\"===============================\")\n",
    "print(f\"Total Reviews: {total_reviews}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total Reviews by Year and Brand -- Top 10 Brands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_by_brand = reviews_df.groupby(['brand','reviewYear'])['review_id'].count() \\\n",
    "                     .reset_index() \\\n",
    "                     .pivot(index='brand', columns='reviewYear', \n",
    "                            values='review_id') \\\n",
    "                     .fillna(0)\n",
    "\n",
    "total_by_brand = reviews_by_brand.sum(axis=1)\n",
    "\n",
    "reviews_by_brand['total']= total_by_brand\n",
    "\n",
    "top_10 = reviews_by_brand.sort_values('total', ascending=False)[:10]\n",
    "\n",
    "top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average Rating by Brand**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_by_brand = reviews_df.groupby(['brand','reviewYear'])['overall'].mean() \\\n",
    "                     .reset_index() \\\n",
    "                     .pivot(index='brand', columns='reviewYear', \n",
    "                            values='overall') \n",
    "\n",
    "overall_avg_by_brand = reviews_df.groupby(['brand'])['overall'].mean() \\\n",
    "                         .reset_index() \\\n",
    "                         .set_index('brand')\n",
    "\n",
    "ratings_by_brand['overall']= overall_avg_by_brand\n",
    "ratings_by_brand.sort_values('overall', ascending=False)\n",
    "ratings_by_brand[ratings_by_brand.index.isin(top_10.index)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of ratings over time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_by_year = reviews_df.groupby(['overall','reviewYear'])['review_id'].count().reset_index() \\\n",
    ".pivot(index='overall', columns='reviewYear', values='review_id')\n",
    "\n",
    "sby_np = stars_by_year.to_numpy()\n",
    "sum_np = sby_np.sum(axis=0)\n",
    "sby_np/sum_np\n",
    "\n",
    "pd.DataFrame(sby_np/sum_np, index=stars_by_year.index, columns=stars_by_year.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Product Dimension Perception Over Time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score_by_topic_year(df):\n",
    "    result_df = df.drop('review_id', axis=1).groupby('reviewYear').mean()\n",
    "    return result_df\n",
    "\n",
    "avg_score_by_topic_year(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an interesting aside, overall star ratings and often sentiment peaked in the 2014/2015 time period. This period also coincided with Amazon's public announcement that it would be cracking down on fake reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Largest Changes in Product Dimension Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_dimension_score_change(reviews_df, start_year, end_year):\n",
    "    topics = []\n",
    "    scores = []\n",
    "    \n",
    "    # Create a dataframe containing only the start and end years\n",
    "    df = avg_score_by_topic_year(reviews_df)\n",
    "    df = df.drop('overall', axis=1)\n",
    "        \n",
    "    df = df.loc[[start_year, end_year]]\n",
    "    \n",
    "    for t in df.columns:\n",
    "        s_score, e_score = tuple(df[t])\n",
    "        scores.append([s_score, e_score, e_score-s_score])\n",
    "        \n",
    "    df = pd.DataFrame(scores, index=df.columns, \n",
    "                      columns=[f\"{start_year}_score\", f\"{end_year}_score\", \"chg_score\"])\n",
    "    \n",
    "    df = df.sort_values(\"chg_score\", ascending=False)\n",
    "    return df\n",
    "    \n",
    "df = overall_dimension_score_change(reviews_df, 2009, 2018)\n",
    "\n",
    "df.sort_values('chg_score', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Changes in Product Dimension Mention Freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cols = [f\"topic_{i}_score\" for i in range(vocab.num_topics)]\n",
    "reviews_w_topics_df = reviews_df[topic_cols].dropna(thresh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_by_topic_year(df):\n",
    "    topic_cols = [f\"topic_{i}_score\" for i in range(vocab.num_topics)]\n",
    "    result_df = df.drop('review_id', axis=1).dropna(thresh=1).groupby('reviewYear').count()\n",
    "    result_df = result_df[topic_cols]\n",
    "    return result_df\n",
    "\n",
    "tmp_df = freq_by_topic_year(reviews_df)\n",
    "tmp_df = tmp_df.loc[[2009,2018]]\n",
    "tmp_np = tmp_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_topic_freq(df, year):\n",
    "    mentioning_topics = df[[f\"topic_{i}_score\" for i in range(vocab.num_topics)]].dropna(thresh=1)\n",
    "    years_df = df['reviewYear'].reset_index().drop('index',axis=1)\n",
    "    years_df = years_df[years_df['reviewYear'] == year]\n",
    "    join_df = years_df.join(mentioning_topics)\n",
    "    n = len(join_df)\n",
    "    tmp_np = join_df.drop('reviewYear',axis=1).count().to_numpy()\n",
    "    return list(tmp_np/n*100)\n",
    "\n",
    "data_2009 = relative_topic_freq(reviews_df, 2009)\n",
    "data_2018 = relative_topic_freq(reviews_df, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_attributes = ['camera',\n",
    "'customer service',\n",
    "'calls & text',\n",
    "'web/email/contacts',\n",
    "'carrier',\n",
    "'apps',\n",
    "'buttons',\n",
    "'call quality',\n",
    "'chargers',\n",
    "'form factor',\n",
    "'screen',\n",
    "'user friendly',\n",
    "'processor speed',\n",
    "'entertainment',\n",
    "'battery',\n",
    "'sim/sd card']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cols = [f\"topic_{i}_score\" for i in range(vocab.num_topics)]\n",
    "top_brands = ['samsung']\n",
    "product_name = 'galaxy s5'\n",
    "\n",
    "product = reviews_df[reviews_df['product'].str.contains(product_name)]\n",
    "reviews_2018 = product[product['reviewYear']==2017].set_index('brand')\n",
    "brands_2018 = reviews_2018[topic_cols].groupby('brand').mean()\n",
    "brands_2018.loc[top_brands].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
